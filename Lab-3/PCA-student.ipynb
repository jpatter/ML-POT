{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#fa04d9\"><center>PRINCIPAL COMPONENT ANALYSIS USING SPARK</center></span>\n",
    "\n",
    "### Suppose we have the following simple dataset composed of 5 datapoints with (x,y) coordinates: A(-4, -1), B(-2 0), C(0, 1), D(2, 2) and E(4, 3)\n",
    "\n",
    "![figure1](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/figure1.jpg)\n",
    "\n",
    "### It is very straightforward to notice that these 5 data points are forming a straight line (with slope equal to 0.5) and could therefore be described with a single dimension vector such as the vector V(2,1) or any multiple of it, as described in the picture below.<br><br>\n",
    "![figure2](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/figure2.jpg)\n",
    "\n",
    "### The figure above represents the straight line (in light blue) passing through points A, B, C, D and E and two example vectors, V of coordinates (2,1) (in red) and U of coordinates (-3, -1.5) (in green) which can serve as a base for a single dimensional set of coordinates for the points on this line.\n",
    "\n",
    "### Let us now suppose that the fact that our data set can be described by a single dimension was not obvious, (this is typically the case with more complex scenarios, where the dataset can be described fairly well with less dimensions than initially given) and work on \"discovering\" this property. This can be done using the mathematical method known as \"Principal Component Analysis\" which is implemented in Apache Spark as \"PCA\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"color:#fa04d9\">**Step 1: Define the Spark context variable**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <span style=\"color:#fa04d9\"> Step 2: Create the dataset corresponding to the five data point described above as a dataframe.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Virtually all Spark machine learning implementations take Vectors of features as input (rather than individual columns), so we will build our data frame as a set of 5 rows, where each row represents one of the data points from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "data = [(Vectors.dense([0.0, 1.0]),), \n",
    "        (Vectors.dense([2.0, 2.0]),),\n",
    "        (Vectors.dense([4.0, 3.0]),),\n",
    "        (Vectors.dense([-2.0, 0.0]),),\n",
    "        (Vectors.dense([-4.0, -1.0]),)\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the correctness of the data frame by displaying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|   features|\n",
      "+-----------+\n",
      "|  [0.0,1.0]|\n",
      "|  [2.0,2.0]|\n",
      "|  [4.0,3.0]|\n",
      "| [-2.0,0.0]|\n",
      "|[-4.0,-1.0]|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Remark: If you are wondering about the syntax used above to create the data frame, where each Vector seems to be \"wrapped\" into an extra set of brackets with an extra comma, you will find below a quick clarification<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">The reason for this is that SparkSession.createDataFrame(), which is used under the hood, requires an RDD / list of Row/tuple/list/ or pandas.DataFrame, unless a schema with DataType is provided. This is better explained with an example.<br> Consider trying to do the following to create a data frame in Python:<br><br>data = ['a', 'b', 'c']<br>df = spark.createDataFrame(data, [\"features\"])<br><br>\n",
    "This code above will fail with a message such as: <span style=\"color:blue\">TypeError: schema should be StructType or list or None, but got: set(['features'])</span> <br><br>\n",
    "You can then consider a couple of ways to fix this.\n",
    "\n",
    "### <span style=\"color:green\">Method 1: Declare the schema type using a StructType directly in the invocation of createDataFrame.\n",
    "<span style=\"color:green\">from pyspark.sql.types import StringType<br> \n",
    "df = spark.createDataFrame(data, StringType(), [\"features\"])\n",
    "\n",
    "### <span style=\"color:green\">Method 2: You can alternatively feed tuples into createDataFrame (rather than single values). This is where the Python syntax to create single element tuples is used. In <span style=\"color:green\">Python, (a,) represents a one-element tuple containing just a.\n",
    "<span style=\"color:green\">data = [('a',), ('b',), ('c',)]<br>\n",
    "df = spark.createDataFrame(data, [\"features\"])<br><br>This code should now be successful.\n",
    "### <span style=\"color:green\">Closing the remark and resuming with the PCA tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 3: Instantiate and train the PCA model in Spark. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will instantiate a standard scaler. The original data will be transformed to have a mean of 0, which is a very common data preparation technic in data science. This implies in our case that our set of 5 data points will be centered around the origin so that the sum of all X values and the sum of all Y values add up to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler_definition = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=False, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As implied by the names used for the variables, scaler_definition is only a definition of the standard scaler. In order to obtain an actual instance, we need to apply the \"fit\" method to the definition,\n",
    "# passing in the actual data.\n",
    "scaler_instance_trained = scaler_definition.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The trained instance of the scaler can now be used with the \"transform\" method, to take in the original dataset (a dataframe of feature vectors) and produce as output a \n",
    "# new dataframe where the data now has a 0 mean.\n",
    "scaler_output_df = scaler_instance_trained.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|   features|scaledFeatures|\n",
      "+-----------+--------------+\n",
      "|  [0.0,1.0]|     [0.0,0.0]|\n",
      "|  [2.0,2.0]|     [2.0,1.0]|\n",
      "|  [4.0,3.0]|     [4.0,2.0]|\n",
      "| [-2.0,0.0]|   [-2.0,-1.0]|\n",
      "|[-4.0,-1.0]|   [-4.0,-2.0]|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify that the newly produced data is indeed scaled to have a 0 mean.\n",
    "scaler_output_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our scaledFeatures, we can create our PCA model which will be expecting an input column named \"scaledFeatures\" and will produce as output a column (of vectors) named \"pcaFeatures\". This notebook will detail the meaning of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Remark:<br>\n",
    "The meaning of the parameter k=2 (in the cell below) indicates that we want our PCA model to produce a description of our data in TWO dimensions. This may seem counter intuitive as we 'already' know that our dataset is a straight line and therefore unidimensional. Remember that we are working with a very simple example where we are pretending not to be aware of the end result. In general, when starting with an N dimensional dataset, it is common to use PCA with a value of N since it is not initially known what the \"meaningful\" number of final dimensions is. As a matter of fact, as we will see shortly, it is one of the outputs of PCA, once it produces a new set of dimensions describing the dataset, to also provide the amount of 'variance' captured by each one of those dimensions. <span style=\"color:red\">**It is then up to the data scientist to decide how many dimensions to keep in order to get the best *bang for the buck* **.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca_model_definition = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same approach as with the standard scaler a few cells above, we create a PCA class object (think of this as a class capable of producing an actual Model, once it has been given a dataset to work with and invoked the 'fit' method on that dataset) and then subsequently make that definition into a *real* instantiated model by fitting it to the output of the standard scaler produced higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_model_instance = pca_model_definition.fit(scaler_output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable \"pca_model_instance\" is the actual PCA model which was obtained using our simple 5 points dataset. **The Spark implementation class type of this object is <span style=\"color:red\">\"PCAModel\"**</span>\n",
    "\n",
    "The Spark documentation provides a list of methods which can be invoked on this pca_model_instance that we just obtained. One method of interest is \"pc\" and stands for \"principal components\". This method returns a matrix which we will look at more closely after displaying it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(2, 2, [-0.8944, -0.4472, -0.4472, 0.8944], 0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_model_instance.pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this matrix has two column/vectors (represented by the first '2' in the output above) and that each column/vector has two coordinates (represented by the second '2'). We can also print this matrix with a slightly better formatting as in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[-0.89442719, -0.4472136 ],\n",
      "             [-0.4472136 ,  0.89442719]])\n"
     ]
    }
   ],
   "source": [
    "print (pca_model_instance.pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 4: Interpreting the output of the PCA implementation in Spark.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp;&nbsp;&nbsp;<span style=\"color:#fa04d9\">Step 4.1: Output of the method \"pc\" from the Spark PCAModel class: The Eigen Vectors. </span>\n",
    "We can represent this matrix as two vectors W and X (or PC1 and PC2, for Principal Component 1 and Principal Component 2 returned by the Spark PCA algorithm) with the corresponding coordinates as can be seen below (coordinates rounded to two decimal significant digits).\n",
    "\n",
    "![eigen vectors](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/eigen_vectors.jpg)\n",
    "\n",
    "The figure below also provides a visual representation of these two dimensions (W and X) returned by PCA, in the context of the original dataset with the five data points A, B, C, D and E and the original vector V(2,1), all represented at a larger scale than the original picture at the beginnig of this notebook. (Note that vector U has been left out to avoid overloading the diagram).\n",
    "![figure3](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/figure3.jpg)\n",
    "\n",
    "### <span style=\"color:blue\">Some important remarks about the results obtained so far:</span><br>\n",
    "1- The dimensions W and X returned by PCA are known mathematically as the <span style=\"color:red\">**\"Eigen vectors\"**</span> of the <span style=\"color:red\">**covariance matrix**</span> of the original dimensions which were provided. **This notebook will not address the mathematical aspects and background of the PCA method although very interesting. Further reading on this topic is very widely available online.**<br><br>\n",
    "2- The first vector / dimension returned by PCA, \"W\" has an exact slope of 0.5 and corresponds to the direction of the vectors U and V which we had identified \"intuitively\" at the beginning of this notebook. For added clarity, vector V has been added to the drawing above to highlight the fact that it has the exact same slope as W (but a different size and direction).<br><br>\n",
    "3- The vector, or dimension X is exactly orthogonal to W. This is a property of the PCA method: all the returned dimensions are orthogonal (even when there are more than two).<br><br>\n",
    "4- The coordinates of vectors W and X were not chosen by the Spark PCA at random (i.e we could pick 'longer' or 'shorter' vectors along the same directions). If you calculate the norm (size) of W or X (given by **sqrt(x^2 + y^2)**), you will notice that each one of those vectors has a norm of 1, making them <span style=\"color:blue\">unit</span> vectors. The <span style=\"color:blue\">basis</span> formed by W and X is therefore <span style=\"color:blue\">orthonormal</span>.<br><br>\n",
    "5- We can arbitrarily multiply anyone of the PCA dimensions by -1, which would only change the overall direction by 180 degrees, but does not change the orthonormal property.<br><br>\n",
    "6- In the <span style=\"color:red\">red</span> orthonormal basis above formed by W and X, we can see that each one of our original datapoints will have a different abscissa (aka X coordinate), but the ordinate (aka Y coordinate) will be rigorously identical for all datapoints. As a matter of fact, if the dataset is initially centered around the origin --imagine shifting the dataset downward in a vertical motion so that datapoint C is at the origin-- (this was performed by the 0 mean transformation of our standard scaler), then we will have the case where the ordinate for each datapoint is <span style=\"color:red\">0</span>.<br><br>\n",
    "7- <span style=\"color:red\">We can conclude from the remark at point 6- just above that the second dimension X is not useful at all, and all datapoints can be identified by their abscissa (or X coordinate) along direction W alone. As a matter of fact, if a dataset is to be used as input to a predictive algorithm, if one dimension presents exactly the same value for all datapoints, then it can be dropped as it does not affect the result at all</span>. This knowledge about how much information is provided by each of the PCA dimensions is known as the <span style=\"color:blue\">**Eigen values**</span> of the same covariance matrix discussed in the first point above. The Spark PCA algorithm provides these **Eigen values** associated with each **Eigen vector** (W and X here), through the method **'explainedVariance'** which we will look at below.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp;&nbsp;&nbsp;<span style=\"color:#fa04d9\">Step 4.2: The \"explainedVariance\" method and the Eigen values:</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the Eigen values for the two dimensions W and X studied so far. This is done using the \"explainedVariance\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_model_instance.explainedVariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have followed the analysis of the PCA dimensions (W and X) results described a couple of cells above, you should hopefully not be surprised by the outpout you are seeing. This is telling us that:**<br>\n",
    "1- The first principal component, or dimension (Eigen vector W in our case) captures <span style=\"color:red\">**100%**</span> of the variance or information in our dataset.<br>\n",
    "2- The second principal component, or dimension (Eigen vector X in our case) caputures <span style=\"color:red\">**0%**</span> of the variance or information in our dataset.<br>\n",
    "3- <span style=\"color:red\">**Consequently, we can infer that our dataset is unidimensional and can be fully described with the single FIRST dimension returned by PCA.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &nbsp;&nbsp;&nbsp;<span style=\"color:#fa04d9\"> Step 4.3: Coordinates of datapoints in the new orthonormal basis returned by PCA </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two different bases in which we can describe our datapoints A, B, C, D and E. The original basis where all points had an X value and a Y value (two dimensions), and the new (red) basis returned by PCA where we have determined that only one vector is going to be sufficient to describe our dataset. The natural subsequent question then becomes: <span style=\"color:red\">**So how do we find the coordinates of datapoints A, B, C, D and E in the new basis which was returned by PCA (represented in red in the figure higher up)?**</span><br><br>\n",
    "Since PCA is a linear transformation of the original dimensions (**details not discussed in this notebook, but details widely available online**), we can obtain the new coordinates of our datapoints in the new basis by multiplying their (original) coordinates by the matrix of Eigen vectors which was discussed a few cells above. You can find in the picture below an example of applying the PCA transformation to the original datapoints A, B, C, D and E\n",
    "\n",
    "![pca_coords](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/pca_coords.jpg)\n",
    "\n",
    "### <span style=\"color:blue\">Some remarks about the matrix multiplication above:</span><br>\n",
    "1- The original datapoints coordinates are collected in the first matrix above, in a transposed (row) format. So point A's coordinates are shown as the first row, point B's coordinates are shown as the second row, and so on...<br><br>\n",
    "2- The PCA matrix with the principal components as it was returned by Spark and discussed higher is represented with a more precise version of the coordinates of the Eigen vectors W and X. Note that this matrix is \"vertical\" and the coordinates of W and X are represented \"vertically\".<br><br>\n",
    "3- The intermediate resulting matrix shows the details of the multiplication operations which take place, and the final matrix (underneath) shows, in each row, the coordinates of the same datapoints from A to E in the target PCA basis.<br><br>\n",
    "4- <span style=\"color:blue\">**As expected, notice how all the datapoints have the same ordinate (Y coordinate) in the new basis, making it therefore useless from a predictive point of view.**</span><br><br>\n",
    "5- <span style=\"color:blue\">**One more detail needs to be addressed.**</span> Remember that the coordinates of points A to E were transformed by the standard scaler to a 0 mean set of values. So the initial matrix of coordinates for the datapoints is actually the one returned as the \"scaledFeatures\" higher towards the beginning of this notebook rather than the one shown in the picture above. Here it is below as a reminder:<br>\n",
    "![scaled_original_coords](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/scaled_original_coords.jpg)\n",
    "\n",
    "6- If we pass these coordinates through the same matrix multiplication as shown above, we get the following results for the PCA coordinates of points A through E:\n",
    "![scaled_pca_coords](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/scaled_pca_coords.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As it turns out, you do not need to perform these matrix multiplications manually. Recall the definition of our PCA model at the beginning of this notebook:\n",
    "pca_model_definition = PCA(k=2, inputCol=\"scaledFeatures\", <span style=\"color:blue\">**outputCol=\"pcaFeatures\"**</span>)<br><br>\n",
    "Spark will actually compute the coordinates of each datapoint in the PCA basis and display them in the output column **pcaFeatures**. Let's take a look at how this works with our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our PCA model is available as \"pca_model_definition\". We have already looked at invoking \"pc\" and \"explainedVariance\" on our model. As with all Spark models, we can also apply this model to the input dataframe of datapoints (using the \"transform\" method) and obtain as a result, an output dataframe that will have appended to it the new coordinates in the PCA dimensions orthonormal basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_output = pca_model_instance.transform(scaler_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0, 1.0]), scaledFeatures=DenseVector([0.0, 0.0]), pcaFeatures=DenseVector([0.0, 0.0])),\n",
       " Row(features=DenseVector([2.0, 2.0]), scaledFeatures=DenseVector([2.0, 1.0]), pcaFeatures=DenseVector([-2.2361, 0.0])),\n",
       " Row(features=DenseVector([4.0, 3.0]), scaledFeatures=DenseVector([4.0, 2.0]), pcaFeatures=DenseVector([-4.4721, 0.0])),\n",
       " Row(features=DenseVector([-2.0, 0.0]), scaledFeatures=DenseVector([-2.0, -1.0]), pcaFeatures=DenseVector([2.2361, 0.0])),\n",
       " Row(features=DenseVector([-4.0, -1.0]), scaledFeatures=DenseVector([-4.0, -2.0]), pcaFeatures=DenseVector([4.4721, 0.0]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see in the cell above:\n",
    "- Each one of the original 5 datapoints as a row in the dataframe.\n",
    "- Both the original and scaled coordinates before the PCA transformation.\n",
    "- The PCA coordinates.\n",
    "- <span style=\"color:blue\">We can also compare for correctness (and better understanding) the results in the pcaFeatures column with the results obtained from the manual matrix multiplication a few cells higher (Scaled PCA Coords).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 5: Visual representations using the Python based Brunel library.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our understanding of the meaning of the pcaFeatures column is solidifying, we can go one step further and use Python capabilities to plot some data. In order to do this, we will use the Brunel library which is extensively described online. If you are interested in learning more about Brunel, one good starting point would be this PDF document:  http://brunel.mybluemix.net/docs/Brunel%20Documentation.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be plotting simple two dimensional graphs, where datapoints have X and Y coordinates. One simple input which Brunel can take in order to produce the desired graph is a Pandas dataframe. (If you are not familiar with the Python Pandas library, you can also find several tutorials online).<br><br> The first step will therefore consist in taking the pcaFeatures output provided by Spark right above and extract it as a Pandas dataframe consisting of two columns \"x\" and \"y\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark transformation below is not the most efficient, but represents a straightforward approach to extracting the desired data from the Spark dataframe and creating a Pandas dataframe named \"my_pandas_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_pandas_data = pca_output.select([\"pcaFeatures\"]).rdd.map(lambda row: (float(row[0][0]), float(row[0][1]))).toDF([\"x\", \"y\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.236068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.472136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.236068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.472136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x  y\n",
       "0  0.000000  0\n",
       "1 -2.236068  0\n",
       "2 -4.472136  0\n",
       "3  2.236068  0\n",
       "4  4.472136  0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pandas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the brunel library\n",
    "import brunel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "  ~ Copyright (c) 2015 IBM Corporation and others.\n",
       "  ~\n",
       "  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "  ~ You may not use this file except in compliance with the License.\n",
       "  ~ You may obtain a copy of the License at\n",
       "  ~\n",
       "  ~     http://www.apache.org/licenses/LICENSE-2.0\n",
       "  ~\n",
       "  ~ Unless required by applicable law or agreed to in writing, software\n",
       "  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  ~ See the License for the specific language governing permissions and\n",
       "  ~ limitations under the License.\n",
       "  -->\n",
       "\n",
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/11a8500e-c783-4f76-b1d4-f5cbd466e13e/nbextensions/brunel_ext/brunel.2.3.css\">\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/11a8500e-c783-4f76-b1d4-f5cbd466e13e/nbextensions/brunel_ext/sumoselect.css\">\n",
       "\n",
       "<style>\n",
       "    \n",
       "</style>\n",
       "\n",
       "<div id=\"controlsiddff5346e-6c32-11e7-a1ad-002590fb7114\" class=\"brunel\"/>\n",
       "<svg id=\"visiddff532b6-6c32-11e7-a1ad-002590fb7114\" width=\"800\" height=\"300\"></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "/*\n",
       " * Copyright (c) 2015 IBM Corporation and others.\n",
       " *\n",
       " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       " * You may not use this file except in compliance with the License.\n",
       " * You may obtain a copy of the License at\n",
       " *\n",
       " *     http://www.apache.org/licenses/LICENSE-2.0\n",
       " *\n",
       " * Unless required by applicable law or agreed to in writing, software\n",
       " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       " * See the License for the specific language governing permissions and\n",
       " * limitations under the License.\n",
       " */\n",
       "\n",
       "require.config({\n",
       "    waitSeconds: 60,\n",
       "    paths: {\n",
       "        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n",
       "        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n",
       "        'brunel' : '/data/jupyter2/11a8500e-c783-4f76-b1d4-f5cbd466e13e/nbextensions/brunel_ext/brunel.2.3.min',\n",
       "        'brunelControls' : '/data/jupyter2/11a8500e-c783-4f76-b1d4-f5cbd466e13e/nbextensions/brunel_ext/brunel.controls.2.3.min'\n",
       "    },\n",
       "    shim: {\n",
       "       'brunel' : {\n",
       "            exports: 'BrunelD3',\n",
       "            deps: ['d3', 'topojson'],\n",
       "            init: function() {\n",
       "               return {\n",
       "                 BrunelD3 : BrunelD3,\n",
       "                 BrunelData : BrunelData\n",
       "              }\n",
       "            }\n",
       "        },\n",
       "       'brunelControls' : {\n",
       "            exports: 'BrunelEventHandlers',\n",
       "            init: function() {\n",
       "               return {\n",
       "                 BrunelEventHandlers: BrunelEventHandlers,\n",
       "                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n",
       "              }\n",
       "            }\n",
       "        }\n",
       "\n",
       "    }\n",
       "\n",
       "});\n",
       "\n",
       "require([\"d3\"], function(d3) {\n",
       "    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n",
       "        function  BrunelVis(visId) {\n",
       "  \"use strict\";                                                                       // strict mode\n",
       "  var datasets = [],                                      // array of datasets for the original data\n",
       "      pre = function(d, i) { return d },                         // default pre-process does nothing\n",
       "      post = function(d, i) { return d },                       // default post-process does nothing\n",
       "      transitionTime = 200,                                        // transition time for animations\n",
       "      charts = [],                                                       // the charts in the system\n",
       "      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n",
       "\n",
       "  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n",
       "\n",
       "  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n",
       "\n",
       "  charts[0] = function(parentNode, filterRows) {\n",
       "    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 43, 37, 13),\n",
       "      elements = [];                                              // array of elements in this chart\n",
       "\n",
       "    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n",
       "\n",
       "    var chart =  vis.append('g').attr('class', 'chart1')\n",
       "      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n",
       "    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n",
       "    var zoom = d3.zoom().scaleExtent([1/3,3]);\n",
       "    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n",
       "      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n",
       "      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n",
       "      .style('cursor', 'move').call(zoom)\n",
       "      .node();\n",
       "    zoomNode.__zoom = d3.zoomIdentity;\n",
       "    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n",
       "    var interior = chart.append('g').attr('class', 'interior zoomNone')\n",
       "      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n",
       "      .attr('clip-path', 'url(#clip_visiddff532b6-6c32-11e7-a1ad-002590fb7114_chart1_inner)');\n",
       "    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n",
       "    var gridGroup = interior.append('g').attr('class', 'grid');\n",
       "    var axes = chart.append('g').attr('class', 'axis')\n",
       "      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n",
       "    vis.append('clipPath').attr('id', 'clip_visiddff532b6-6c32-11e7-a1ad-002590fb7114_chart1_inner').append('rect')\n",
       "      .attr('x', 0).attr('y', 0)\n",
       "      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n",
       "\n",
       "    // Scales //////////////////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "    var scale_x = d3.scaleLinear().domain([-5, 5.000001])\n",
       "      .range([0, geom.inner_width]);\n",
       "    var scale_inner = d3.scaleLinear().domain([0,1])\n",
       "      .range([-0.5, 0.5]);\n",
       "    var scale_y = d3.scaleLinear().domain([0, 1.0000001])\n",
       "      .range([geom.inner_height, 0]);\n",
       "    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n",
       "\n",
       "    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "    axes.append('g').attr('class', 'x axis')\n",
       "      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n",
       "      .attr('clip-path', 'url(#clip_visiddff532b6-6c32-11e7-a1ad-002590fb7114_chart1_haxis)');\n",
       "    vis.append('clipPath').attr('id', 'clip_visiddff532b6-6c32-11e7-a1ad-002590fb7114_chart1_haxis').append('polyline')\n",
       "      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n",
       "    axes.select('g.axis.x').append('text').attr('class', 'title').text('X').style('text-anchor', 'middle')\n",
       "      .attr('x',geom.inner_rawWidth/2)\n",
       "      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n",
       "    axes.append('g').attr('class', 'y axis')\n",
       "      .attr('clip-path', 'url(#clip_visiddff532b6-6c32-11e7-a1ad-002590fb7114_chart1_vaxis)');\n",
       "    vis.append('clipPath').attr('id', 'clip_visiddff532b6-6c32-11e7-a1ad-002590fb7114_chart1_vaxis').append('polyline')\n",
       "      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n",
       "    axes.select('g.axis.y').append('text').attr('class', 'title').text('Y').style('text-anchor', 'middle')\n",
       "      .attr('x',-geom.inner_rawHeight/2)\n",
       "      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n",
       "\n",
       "    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 33.0)));\n",
       "    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n",
       "\n",
       "    function buildAxes(time) {\n",
       "      var axis_x = axes.select('g.axis.x');\n",
       "      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n",
       "      var axis_y = axes.select('g.axis.y');\n",
       "      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n",
       "    }\n",
       "    zoom.on('zoom', function(t, time) {\n",
       "        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n",
       "        scale_x = t.rescaleX(base_scales[0]);\n",
       "        scale_y = t.rescaleY(base_scales[1]);\n",
       "        zoomNode.__zoom = t;\n",
       "        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n",
       "        build(time || -1);\n",
       "    });\n",
       "\n",
       "    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "    elements[0] = function() {\n",
       "      var original, processed,                           // data sets passed in and then transformed\n",
       "        element, data,                                 // brunel element information and brunel data\n",
       "        selection, merged;                                      // d3 selection and merged selection\n",
       "      var elementGroup = interior.append('g').attr('class', 'element1'),\n",
       "        main = elementGroup.append('g').attr('class', 'main'),\n",
       "        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n",
       "\n",
       "      function makeData() {\n",
       "        original = datasets[0];\n",
       "        if (filterRows) original = original.retainRows(filterRows);\n",
       "        processed = pre(original, 0);\n",
       "        processed = post(processed, 0);\n",
       "        var f0 = processed.field('x'),\n",
       "          f1 = processed.field('y'),\n",
       "          f2 = processed.field('#selection'),\n",
       "          f3 = processed.field('#row');\n",
       "        var keyFunc = function(d) { return f3.value(d) };\n",
       "        data = {\n",
       "          x:            function(d) { return f0.value(d.row) },\n",
       "          y:            function(d) { return f1.value(d.row) },\n",
       "          $selection:   function(d) { return f2.value(d.row) },\n",
       "          $row:         function(d) { return f3.value(d.row) },\n",
       "          x_f:          function(d) { return f0.valueFormatted(d.row) },\n",
       "          y_f:          function(d) { return f1.valueFormatted(d.row) },\n",
       "          $selection_f: function(d) { return f2.valueFormatted(d.row) },\n",
       "          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n",
       "          _split:       function(d) { return f2.value(d.row) },\n",
       "          _key:         keyFunc,\n",
       "          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n",
       "        };\n",
       "      }\n",
       "      // Aesthetic Functions\n",
       "      var scale_color = d3.scaleOrdinal()\n",
       "        .domain(['✗', '✓'])\n",
       "        .range([ '#00538A', '#C10020', '#F4C800', '#007D34', '#803E75', '#FF6800', \n",
       "          '#817066', '#FFB300', '#F6768E', '#93AA00', '#53377A', '#FF8E00', '#B32851', \n",
       "          '#CEA262', '#FF7A5C', '#7F180D', '#593315', '#F13A13', '#232C16']);\n",
       "      var color = function(d) { return scale_color(data.$selection(d)) };\n",
       "\n",
       "      // Build element from data ///////////////////////////////////////////////////////////////////\n",
       "\n",
       "      function build(transitionMillis) {\n",
       "        element = elements[0];\n",
       "        var w = Math.abs( scale_x(scale_x.domain()[0] + 2.2360679775) - scale_x.range()[0] );\n",
       "        var x = function(d) { return scale_x(data.x(d))};\n",
       "        var h = geom.default_point_size;\n",
       "        var y = function(d) { return scale_y(data.y(d))};\n",
       "\n",
       "        // Define selection entry operations\n",
       "        function initialState(selection) {\n",
       "          selection\n",
       "            .attr('class', 'element point filled')\n",
       "            .style('pointer-events', 'none')\n",
       "        }\n",
       "\n",
       "        // Define selection update operations on merged data\n",
       "        function updateState(selection) {\n",
       "          selection\n",
       "            .attr('cx',function(d) { return scale_x(data.x(d))})\n",
       "            .attr('cy',function(d) { return scale_y(data.y(d))})\n",
       "            .attr('r',Math.min(Math.abs( scale_x(scale_x.domain()[0] + 2.2360679775) - scale_x.range()[0] ), geom.default_point_size) / 2)\n",
       "            .filter(BrunelD3.hasData)                     // following only performed for data items\n",
       "            .style('fill', color);\n",
       "        }\n",
       "\n",
       "        // Define labeling for the selection\n",
       "        function label(selection, transitionMillis) {\n",
       "        }\n",
       "        // Create selections, set the initial state and transition updates\n",
       "        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n",
       "        var added = selection.enter().append('circle');\n",
       "        merged = selection.merge(added);\n",
       "        initialState(added);\n",
       "        selection.filter(BrunelD3.hasData)\n",
       "          .classed('selected', BrunelD3.isSelected(data))\n",
       "          .filter(BrunelD3.isSelected(data)).raise();\n",
       "        updateState(BrunelD3.transition(merged, transitionMillis));\n",
       "\n",
       "        BrunelD3.transition(selection.exit(), transitionMillis/3)\n",
       "          .style('opacity', 0.5).each( function() {\n",
       "            this.remove(); BrunelD3.removeLabels(this); \n",
       "        });\n",
       "      }\n",
       "\n",
       "      return {\n",
       "        data:           function() { return processed },\n",
       "        original:       function() { return original },\n",
       "        internal:       function() { return data },\n",
       "        selection:      function() { return merged },\n",
       "        makeData:       makeData,\n",
       "        build:          build,\n",
       "        chart:          function() { return charts[0] },\n",
       "        group:          function() { return elementGroup },\n",
       "        fields: {\n",
       "          x:            ['x'],\n",
       "          y:            ['y'],\n",
       "          key:          ['#row'],\n",
       "          color:        ['#selection']\n",
       "        }\n",
       "      };\n",
       "    }();\n",
       "\n",
       "    function build(time, noData) {\n",
       "      var first = elements[0].data() == null;\n",
       "      if (first) time = 0;                                           // no transition for first call\n",
       "      buildAxes(time);\n",
       "      if ((first || time > -1) && !noData) {\n",
       "        elements[0].makeData();\n",
       "      }\n",
       "      elements[0].build(time);\n",
       "    }\n",
       "\n",
       "    // Expose the following components of the chart\n",
       "    return {\n",
       "      elements : elements,\n",
       "      interior : interior,\n",
       "      scales: {x:scale_x, y:scale_y},\n",
       "      zoom: function(params, time) {\n",
       "          if (params) zoom.on('zoom').call(zoomNode, params, time);\n",
       "          return d3.zoomTransform(zoomNode);\n",
       "      },\n",
       "      build : build\n",
       "    };\n",
       "    }();\n",
       "\n",
       "  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n",
       "  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n",
       "  function buildAll() {\n",
       "    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n",
       "    updateAll(transitionTime);\n",
       "  }\n",
       "\n",
       "  return {\n",
       "    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n",
       "    dataPostProcess:    function(f) { if (f) post = f; return post },\n",
       "    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n",
       "    visId:              visId,\n",
       "    build:              buildAll,\n",
       "    rebuild:            updateAll,\n",
       "    charts:             charts\n",
       "  }\n",
       "}\n",
       "\n",
       "// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "var table1 = {\n",
       "   summarized: false,\n",
       "   names: ['x', 'y'], \n",
       "   options: ['numeric', 'numeric'], \n",
       "   rows: [[0, 0], [-2.236068, 0], [-4.472136, 0], [2.236068, 0], [4.472136, 0]]\n",
       "};\n",
       "\n",
       "// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n",
       "\n",
       "var v  = new BrunelVis('visiddff532b6-6c32-11e7-a1ad-002590fb7114');\n",
       "v.build(table1);\n",
       "\n",
       "    });\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the 5 datapoints\n",
    "%brunel data('my_pandas_data') point x(x) y(y) color(#selection):: width=800, height=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:<br> \n",
    "We have seen in this first simple example how PCA can be used to reduce the dimensionality of a problem / dataset. In this example above, if the initial dataset was to be used as input to a predictive algorithm, we could reduce the input to a single dimension through PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 6: 2D exercise.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**Insert a few blank cells below in this notebook and modify your datapoints in such a way that the resulting dataset is not aligned anymore and then rerun the principal component analysis as shown above (including the brunel visualizations)**:\n",
    "- Which significant change do you anticipate to observe in comparison with the example covered so far ?\n",
    "- How many dimensions do you need to describe your new dataset ? \n",
    "- Would you still keep one single dimension if your data was being used for predictive purposes ? Why or Why not ?\n",
    "- Using brunel, plot your original data and the transformation into the PCA dimension(s)\n",
    "- Discuss your results with you neighbor or instructor at your preference.\n",
    "- **Stretch goal: Keep modifying your dataset in specific ways, such as giving it a particular shape (elongated in one particular direction, etc...) and keep running PCA and check how the algorithm will systematically extract the first dimension as the direction in which your dataset has the most information.****</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here is one suggested modified dataset to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [(Vectors.dense([-4.0, -2.0]),),\n",
    "        (Vectors.dense([-2.0, 0.5]),),\n",
    "        (Vectors.dense([1.0, 1.0]),),\n",
    "        (Vectors.dense([1.0, -2.0]),),\n",
    "        (Vectors.dense([2.0, 0.0]),),\n",
    "        (Vectors.dense([4.0, 2.0]),)\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Your exercise space goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fa04d9\">Step 6: 3D exercise.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**We have so far worked in this notebook with basic two dimensional datasets in order to get a grasp of PCA. However, as one might suspect, PCA provides added value when working with higher numbers of dimensions, although it becomes more difficult to represent things visually as the number of dimensions increases. In the exercise suggested below, it is proposed to download a three dimensional dataset and perform a PCA analysis on it to determine whether it may be simplified and how.<br><br>\n",
    "<span style=\"color:blue\">Suggested exercise steps:<br>\n",
    "1- Download the threed dataset (code provided in the cell below)<br>\n",
    "2- Define a vector assembler.<br>\n",
    "3- Define a standard scaler. <br>\n",
    "4- Build a pipeline with those two transformers: StandardScaler and VectorAssembler.<br>\n",
    "5- Define an instance of PCA and run it on your dataset.<br>\n",
    "6- Determine how many dimensions you can drop and how many you would keep, explain why.<br>\n",
    "7- What can you say about the \"shape\" of the dataset based on the Eigen values you are seeing?<br>\n",
    "8- Confirm your conclusions from the step above by displaying the result of your PCA using Brunel.**<br></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below will download a three dimensional dataset from github (deletes any existing local version of the file before downloading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-07-18 22:33:24--  https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/data/threed_data.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 559 [text/plain]\n",
      "Saving to: ‘threed_data.csv’\n",
      "\n",
      "100%[======================================>] 559         --.-K/s   in 0s      \n",
      "\n",
      "2017-07-18 22:33:25 (151 MB/s) - ‘threed_data.csv’ saved [559/559]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Name=u'P1', X=0.0, Y=0.0, Z=2.0),\n",
       " Row(Name=u'P2', X=0.0, Y=-2.0, Z=0.0),\n",
       " Row(Name=u'P3', X=2.0, Y=0.0, Z=0.0),\n",
       " Row(Name=u'P4', X=-0.47, Y=-0.63, Z=1.84),\n",
       " Row(Name=u'P5', X=0.63, Y=0.47, Z=1.84)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete the file if it exists, download a new copy from GitHub and load it into a dataframe\n",
    "!rm threed_data.csv* -f\n",
    "!wget https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/data/threed_data.csv\n",
    "\n",
    "threed_data = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('./threed_data.csv')\n",
    "\n",
    "# Take a look at a few elements of the dataset.\n",
    "threed_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your exercise space starts here:\n",
    "## Some hints are provided below, covering SparkML concepts mentioned in the exercise steps above, but not covered so far in this notebook, such as <span style=\"color:blue\">VectorAssembler</span> and <span style=\"color:blue\">Pipeline</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Step 2</span> of the recommended steps above suggests creating a <span style=\"color:blue\">VectorAssembler.</span> VectorAssemblers are standard SparkML transformers which have not been previously covered in this notebook. If you are not familiar with this concept, the hints below help you define a vector assembler that will take the three coordinates X, Y, Z of each point in your dataset and concatenate them into a single vector. <br><br>The first hint below is generic, the second hint is more directed at the code you need to write for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-14\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-14\" href=\"#collapse1-14\">\n",
    "        Hint1: Click on this link to expand this cell, then copy and paste the code which will appear in a new cell just below, and execute that cell to see how VectorAssembler works. (You may subsequently delete that new cell and proceed with the exercise).</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-14\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "from pyspark.ml.linalg import Vectors <br>\n",
    "from pyspark.ml.feature import VectorAssembler <br>\n",
    "<br>\n",
    "dataset = spark.createDataFrame( <br>\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)], <br>\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"]) <br>\n",
    "<br>\n",
    "assembler = VectorAssembler( <br>\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"], <br>\n",
    "    outputCol=\"features\") <br>\n",
    "<br>\n",
    "output = assembler.transform(dataset) <br>\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\") <br>\n",
    "output.select(\"features\", \"clicked\").show(truncate=False) <br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-1\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-1\">\n",
    "        Hint2: If you desire additional help creating a vector assembler for you current dataset, click here to unveil the code to define a VectorAssembler, which you can copy/paste in a new cell below.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-1\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "      # Import VectorAssembler, we haven't used it yet.<br>\n",
    "      from pyspark.ml.feature import VectorAssembler <br>\n",
    "      featureCols = ['X', 'Y', 'Z'] <br>\n",
    "      assembler_definition = VectorAssembler(inputCols=featureCols , outputCol=\"features\")<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Step 3</span> of the recommended steps above suggests creating a <span style=\"color:blue\">StandardScaler.</span> There are such examples higher in this notebook which you can duplicate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-2\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse1-2\">\n",
    "        Hint3: If you desire additional help creating a Standard Scaler for you current dataset, click here to unveil the code to define it, which you can copy/paste in a new cell below.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "   from pyspark.ml.feature import StandardScaler <br>\n",
    "   scaler_definition = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",<br>\n",
    "                        withStd=False, withMean=True) <br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue\">Step 4</span> of the recommended steps above suggests creating a <span style=\"color:blue\">Pipeline.</span> If you are not familiar with this concept, the hint below will help you define a pipeline combining the VectorAssembler and StandardScaler (just defined above), which will take your input data and produce a scaled vector ready to be fed into the PCA logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel-group\" id=\"accordion-4\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse1-4\">\n",
    "        Hint4: If you desire additional help creating a Pipeline combining your VectorAssembler and Standard Scaler, click here to unveil the code to define it, which you can copy/paste in a new cell below.</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">\n",
    "       from pyspark.ml import Pipeline<br>\n",
    "       <br>\n",
    "       pipeline_data_prep_definition = Pipeline(stages=[assembler_definition, scaler_definition]) <br>\n",
    "       pipeline_data_prep_instance = pipeline_data_prep_definition.fit(threed_data) <br>\n",
    "       prepped_data_for_pca = pipeline_data_prep_instance.transform(threed_data)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you have a dataframe (as output of the Pipeline) with data prepped for PCA (make sure you display a few rows from your dataframe to confirm you have the desired format), the rest of the steps of this exercise are very similar to examples previously covered in this notebook. Feel free to include additional blank cells below for the rest of your work.\n",
    "### Please use the <span style=\"color:blue\">\"suggested steps\"</span> at the beginning of this exercise for guidance. Your next step should be:<br> <span style=\"color:blue\">'step 5': defining an instane of PCA and running it on your prepared dataset.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cell below can be the last in your notebook to delete the datafile which was downloaded at the beginning of the exercise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the local file (not needed anymore)\n",
    "!rm threed_data.csv* -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.0",
   "language": "python",
   "name": "python2-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
